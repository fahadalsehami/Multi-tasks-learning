{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cdc_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uAe_4KpL8vEG",
        "wY44Enw79F5i",
        "uOl1AFVD9liL",
        "Bu5gCM4F9r2J",
        "bD6Y3WrIANWV",
        "5xb35YSxAk2I",
        "jcaGMW3T3L1S",
        "a5fh6lY0CImK",
        "BA_OkLByqC2X",
        "9f-GcrYLCjzh",
        "hxLwpQvVxS5t",
        "4EaUd1M-WTzM",
        "NXRihjWdY6pN",
        "was9PkegDHJQ",
        "BvLWeA4Hk5ZG",
        "euuu6NIVlYjN",
        "nVCnOwvtn4Zu",
        "BxT3TxZUZUfp",
        "a6gH_rdGh9mS",
        "8HHQBB7b-EQ9",
        "khRcR3tel5rR",
        "A0T9YuapKXR7",
        "NfNeGyWiKhAs",
        "jxV3SuwEKiWL",
        "SQv0gMfNKmY1",
        "Imkf69MqLR_J",
        "6NoInGItOKeE",
        "esV9tcdAOL1Y"
      ],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rj-73ga5q5js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdcution\n",
        "\n",
        "The purpose of this notebook is to provide a detailed socio-demographic profile of heart-disease mortality rates across state/county in the United States."
      ],
      "metadata": {
        "id": "uAe_4KpL8vEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library required"
      ],
      "metadata": {
        "id": "wY44Enw79F5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import os #Operating system library\n",
        "import pandas as pd #data science framework library\n",
        "import json #json format library\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import glob\n",
        "!pip install dask  \n",
        "!pip install \"dask[complete]\"\n",
        "from dask import dataframe as dd\n",
        "\n",
        "!pip install pycountry_convert \n",
        "#Print style setup\n",
        "from IPython.display import display_html\n",
        "def display_side_by_side(*args):\n",
        "    html_str=''\n",
        "    for df in args:\n",
        "        html_str+=df.to_html()\n",
        "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
        "\n",
        "\n",
        "from matplotlib import ticker \n",
        "# import pycountry_convert as pc\n",
        "import folium\n",
        "import branca\n",
        "from datetime import datetime, timedelta,date\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "import plotly.express as px\n",
        "import json, requests\n",
        "#import calmap\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.layers import Input, Dense, Activation, LeakyReLU\n",
        "from keras import models\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "5QkTB6rSFx-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "USRuVTDD4EDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oe9vSss2OPk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading dataset"
      ],
      "metadata": {
        "id": "uOl1AFVD9liL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Column used in the cdc dataset\n",
        "cols = ['resident_status',\n",
        "       'month_of_death', 'sex','detail_age',\n",
        "       'place_of_death_and_decedents_status', 'marital_status',\n",
        "       'day_of_week_of_death', 'current_data_year', 'injury_at_work',\n",
        "       'manner_of_death', 'autopsy',\n",
        "       '39_cause_recode',\n",
        "       'education_2003_revisionD', 'raceD', 'FIPS']\n",
        "\n",
        "#Reading the original CDC dataset as is from the parent source:\n",
        "df2015 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/CDC_2015_fipsImputed.csv', delimiter=',', usecols = cols)\n",
        "df2015 = df2015.dropna(how = 'any')\n",
        "\n",
        "\n",
        "  #We have selected 'GEOID' as refrence variable \n",
        "cp_cols = ['index', 'TBLID','GEOID', 'GEONAME','PROFTBL','PROFLN', 'TITLE', 'EST_1418','EST_0913','SIG90_1418_0913']\n",
        "#Census datase\n",
        "cp02 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/cp02.csv', encoding='latin-1')\n",
        "cp03 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/cp03.csv', encoding='latin-1')\n",
        "cp04 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/cp04.csv', encoding='latin-1')\n",
        "cp05 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/cp05.csv', encoding='latin-1')\n",
        "\n",
        "#Concatenate datasets\n",
        "cp_df1 = pd.concat([cp02, cp03, cp04, cp05])\n",
        "\n",
        "#We will drop most of the features na dwill choose the target variable as primary key `Provider Zip Code` along wiht features we think that they are important for social determanats \n",
        "flcols = ['Provider Zip Code', 'County', 'Provider City', 'Number of All Beds',\n",
        "          'Total Number of Occupied Beds','Able to Test or Obtain Resources to Test All Current Residents Within Next 7 Days',\n",
        "          'Able to Test or Obtain Resources to Test All Staff and/or Personnel Within Next 7 Days','Shortage of Nursing Staff',\n",
        "          'Shortage of Clinical Staff', 'Number of Residents Staying in this Facility for At Least 1 Day This Week']\n",
        "          \n",
        "#Data.CMS.gov dataset for 2020, 2021, 2022\n",
        "fl2020 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/faclevel_2020.csv', usecols = flcols, header=0)\n",
        "fl2021 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/faclevel_2021.csv', usecols = flcols, header=0)\n",
        "# fl2022 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/faclevel_2022.csv', usecols = flcols) #No values \n",
        "\n",
        "#Concatenate datasets\n",
        "fclevel_df1 = pd.concat([fl2020, fl2021], axis=1)\n",
        "\n",
        "\n",
        "#plc_cols= ['Year',\t'StateAbbr',\t'StateDesc',\t'LocationName',\t'Category',\t'Measure',\t'TotalPopulation',\t'MeasureId',\t'DataValueTypeID','Data_Value_Unit',\t'Short_Question_Text', 'Geolocation']\n",
        "\n",
        "\n",
        "plc2020 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/PLACES__Local_Data_for_Better_Health__County_Data_2020_release.csv',  header = 0)\n",
        "plc2021 = pd.read_csv('/content/gdrive/MyDrive/CDC/cdc_dataset/PLACES__Local_Data_for_Better_Health__County_Data_2021_release.csv', header = 0)\n",
        "\n",
        "\n",
        "#Concatenate datasets\n",
        "plc_df1 = pd.concat([plc2020, plc2021])"
      ],
      "metadata": {
        "id": "cMaI5Q0EFpnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2015.shape"
      ],
      "metadata": {
        "id": "kyF5Uh0747VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Bu5gCM4F9r2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the diminsion of dataset we\n",
        "#Print name of columns \n",
        "cols1 = list(df2015.columns)\n",
        "cols2 = list(cp_df1.columns)\n",
        "cols3 = list(fclevel_df1.columns)\n",
        "cols4 = list(plc_df1.columns)\n",
        "print(f'df2015 Columns are {cols1} and\\n\\n cp_df1 Columns {cols2}\\n\\n fclevel_df1 Columns and {cols3}\\n\\n plc_df1 Columns and {cols4}\\n\\n')\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "ARMB3ZF7HwQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print data shape\n",
        "nRow, nCol = df2015.shape\n",
        "nRow2, nCol2 = cp_df1.shape\n",
        "nRow3, nCol3 = fclevel_df1.shape\n",
        "nRow4, nCol4 = plc_df1.shape\n",
        "\n",
        "print(f'df2015 has {nRow} rows and {nCol} columns and \\n\\n cp_df1 has {nRow2} rows and {nCol2} columns and \\n\\n fclevel_df1 has {nRow3} rows and {nCol3} columns and \\n\\n plc_df1 has {nRow4} rows and {nCol4} columns')"
      ],
      "metadata": {
        "id": "uoFXiLtqS8iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Print data type of datasets\n",
        "dfs = [df2015, cp_df1,fclevel_df1,plc_df1]\n",
        "\n",
        "for df in dfs:\n",
        "    print(df.dtypes)"
      ],
      "metadata": {
        "id": "Fv-PlPgJc4L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cleaning"
      ],
      "metadata": {
        "id": "bD6Y3WrIANWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## df2015"
      ],
      "metadata": {
        "id": "5xb35YSxAk2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "print(tabulate(df2015[1:3], headers = cols))"
      ],
      "metadata": {
        "id": "K062-MheTFQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Missing data\n",
        "# total = df2015.isnull().sum().sort_values(ascending=False)\n",
        "# percent = (df2015.isnull().sum()/df2015.isnull().count()).sort_values(ascending=False)\n",
        "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "# missing_data.head(20)"
      ],
      "metadata": {
        "id": "IxQEOq3KOXmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check duplicate rows\n",
        "df2015.duplicated().sum()"
      ],
      "metadata": {
        "id": "TiP9fDv1c3_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exctracting duplicate rows\n",
        "df2015.loc[df2015.duplicated(),:]"
      ],
      "metadata": {
        "id": "3RicJ-xFdUc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Duplication Rows\n",
        "df2015.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "-NfJQj2GeOl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2015.shape"
      ],
      "metadata": {
        "id": "ovxoTlJEe_5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2258768-2257564\n",
        "#1204 rows have been drpped"
      ],
      "metadata": {
        "id": "qceGzmxlf-6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2015.columns"
      ],
      "metadata": {
        "id": "qfZs1ftbqNac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename and use lowercapital letters\n",
        "df2015 = df2015.rename(columns={'detail_age':'age', 'place_of_death_and_decedents_status':'place_of_death','education_2003_revisionD':'education', '39_cause_recode':'heart_disease','raceD':'race'})\n",
        "\n",
        "#Profiling construction\n",
        "#Education profile\n",
        "df2015['education'].loc[df2015['education']==1] = '8_grade_and_less'\n",
        "df2015['education'].loc[df2015['education']==2] = '9_12grade'\n",
        "df2015['education'].loc[df2015['education']==3] = 'highschool'\n",
        "df2015['education'].loc[df2015['education']==4] = 'college_credit'\n",
        "df2015['education'].loc[df2015['education']==5] = 'associate_degree'\n",
        "df2015['education'].loc[df2015['education']==6] = 'bachelor'\n",
        "df2015['education'].loc[df2015['education']==7] = 'master'\n",
        "df2015['education'].loc[df2015['education']==8] = 'doctor'\n",
        "\n",
        "#Race profile\n",
        "df2015['race'].loc[df2015['race']==1] = 'White'\n",
        "df2015['race'].loc[df2015['race']==2] = 'Other'\n",
        "df2015['race'].loc[df2015['race']==3] = 'Black'\n",
        "\n",
        "#Resident profile\n",
        "df2015['resident_status'].loc[df2015['resident_status'] == 1] = 1 #Resident \n",
        "df2015['resident_status'].loc[df2015['resident_status'] != 4] = 0  #Foreign resident\n",
        "\n",
        "#Quarter profile\n",
        "#Month of death aggregated from number of months to quarter per a year\n",
        "df2015['quarter'] = ''\n",
        "df2015['quarter'].loc[(df2015['month_of_death']==1)|\n",
        "                      (df2015['month_of_death']==2)|\n",
        "                      (df2015['month_of_death']==3)] = 'Q1'#First Quarter \n",
        "\n",
        "df2015['quarter'].loc[(df2015['month_of_death']==4)|\n",
        "                      (df2015['month_of_death']==5)|\n",
        "                      (df2015['month_of_death']==6)] = 'Q2'#Second Quarter\n",
        "\n",
        "df2015['quarter'].loc[(df2015['month_of_death']==7)|\n",
        "                      (df2015['month_of_death']==8)|\n",
        "                      (df2015['month_of_death']==9)] = 'Q3'#Third Quarter\n",
        "\n",
        "df2015['quarter'].loc[(df2015['month_of_death']==10)|\n",
        "                      (df2015['month_of_death']==11)|\n",
        "                      (df2015['month_of_death']==12)] = 'Q4'#Fourth Quarter\n",
        "\n",
        "#Place of death profile \n",
        "df2015['place_of_death']=df2015['place_of_death'].astype(str) #change 'place_of_death' to be string \n",
        "#Places of deaths profile\n",
        "df2015['place_of_death'].loc[df2015['place_of_death']==3] = 'Medical_Center'\n",
        "df2015['place_of_death'].loc[df2015['place_of_death']==4] = 'Home'\n",
        "df2015['place_of_death'].loc[df2015['place_of_death']==5] = 'Hospice'\n",
        "df2015['place_of_death'].loc[df2015['place_of_death']==6] = 'NursingHome'\n",
        "df2015['place_of_death'].loc[df2015['place_of_death']==7] = 'Other'\n",
        "\n",
        "#manner of death profile\n",
        "df2015['manner_of_death'].loc[df2015['manner_of_death']==1] = 'Accidents'\n",
        "df2015['manner_of_death'].loc[df2015['manner_of_death']==2] = 'Suicide'\n",
        "df2015['manner_of_death'].loc[df2015['manner_of_death']==3] = 'Homicide'\n",
        "df2015['manner_of_death'].loc[df2015['manner_of_death']==4] = 'Pending_investigation'\n",
        "df2015['manner_of_death'].loc[df2015['manner_of_death']==6] = 'Self-Inflicted'\n",
        "df2015['manner_of_death'].loc[df2015['manner_of_death']==7] = 'Natural'\n",
        "\n",
        "#Patient with autopsy profile\n",
        "df2015['autopsy'].loc[df2015['autopsy'] == 'Y'] = 1 #Yes patient get autopsy\n",
        "df2015['autopsy'].loc[df2015['autopsy'] == 'y'] = 1 ##Yes patient get autopsy\n",
        "df2015['autopsy'].loc[df2015['autopsy'] == 'N'] = 0 #No patient get autopsy\n",
        "df2015['autopsy'].loc[df2015['autopsy'] == 'n'] = 0 #No patient get autopsy\n",
        "\n",
        "#Gender profile\n",
        "df2015['sex'].loc[df2015['sex'] == 'M'] = 1 #Male\n",
        "df2015['sex'].loc[df2015['sex'] == 'F'] = 0 #Female\n",
        "\n",
        "\n",
        "#Conditions construction\n",
        "\n",
        "#Patient wit Heart Disease Condition profile:\n",
        "df2015['heart_disease']=df2015['heart_disease'].astype(int) #Convert String to Int\n",
        "\n",
        "\n",
        "heart_hd = df2015['heart_disease']\n",
        "heart_hd.loc[heart_hd == 1] = 0\n",
        "heart_hd.loc[(heart_hd >= 19)&(heart_hd <= 22)] = 1 #Patient has heart disease leis between the row 19 and 22\n",
        "heart_hd.loc[heart_hd != 1] = 0\n",
        "\n",
        "df2015 = df2015.drop(['heart_disease'],axis = 1)\n",
        "df2015 = pd.concat([df2015,heart_hd],axis = 1)\n",
        "\n",
        "\n",
        "#Remove  methods\n",
        "df2015.dropna(axis = 0, how = 'any', inplace = True)\n",
        "df2015 = df2015[df2015['autopsy'] != 'U'] #U is uknown \n",
        "df2015 = df2015[df2015['education'] != 9] # number 9 is uknown \n",
        "df2015 = df2015[df2015['marital_status'] != 'U'] #U is uknown \n",
        "df2015 = df2015[df2015['injury_at_work']!= 'U'] #U is uknown \n",
        "df2015 = df2015[df2015['age'] != 120] #number 120 as years old\n",
        "df2015 = df2015[df2015['place_of_death'] != 9] # number 9 is uknown "
      ],
      "metadata": {
        "id": "Un_q9-71BYR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulize the patient recorded in the dataaset as part of the explotary analysis \n",
        "\n",
        "#Create new feature called 'hd_death' which show those who's deaths are caused by heart disease.\n",
        "hd_death = df2015[df2015['heart_disease'] == 1]\n",
        "plt.figure(figsize = (20,16))\n",
        "print('Heart disease caused death are {}% of the total death'.format(str(100*round((hd_death['heart_disease'].value_counts()[1])/len(df2015['heart_disease']),2))))\n",
        "\n",
        "#Plot Age\n",
        "plt.subplot2grid((4,3),(0,0)) \n",
        "hd_death.age.value_counts().plot(kind='line',label = 'Heart Disease',color = 'red')\n",
        "df2015.age.value_counts().plot(kind='line',label = 'Total',color = 'grey')\n",
        "#plt.xlim(0,20)\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.title('Age distribution')\n",
        "plt.ylabel('number')\n",
        "\n",
        "#Plot Sex\n",
        "plt.subplot2grid((3,3),(0,1)) \n",
        "x_sex = hd_death.sex.unique()\n",
        "plt.bar(x=x_sex, height=df2015.sex.value_counts(), label='Total', color='grey', alpha=0.8)\n",
        "plt.bar(x=x_sex, height=hd_death.sex.value_counts(), label='Heart Disease', color='pink', alpha=0.8)\n",
        "\n",
        "for x in x_sex:\n",
        "    percent = round(hd_death.sex.value_counts()[x]/df2015.sex.value_counts()[x],2)\n",
        "    plt.text(x, hd_death.sex.value_counts()[x] + 150, '%s' % (str(int(percent*100))+'%'), ha='center', va='bottom')\n",
        "for x,y in zip(x_sex,hd_death.sex.value_counts()):\n",
        "   plt.text(x, y + 150, '%s' % y, ha='center', va='top')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.xticks(x_sex)\n",
        "plt.title('Sex distribution')\n",
        "plt.ylabel('number')\n",
        "\n",
        "#Plot Marital Status\n",
        "plt.subplot2grid((3,3),(0,2))\n",
        "x_ms = hd_death.marital_status.unique()\n",
        "plt.bar(x=x_ms, height=df2015.marital_status.value_counts(), label='Total', color='grey', alpha=0.8)\n",
        "plt.bar(x=x_ms, height=hd_death.marital_status.value_counts(), label='Heart Disease', color='lightblue', alpha=0.8)\n",
        "\n",
        "for x in x_ms:\n",
        "    percent = round(hd_death.marital_status.value_counts()[x]/df2015.marital_status.value_counts()[x],2)\n",
        "    plt.text(x, hd_death.marital_status.value_counts()[x] + 150, '%s' % str(int(percent*100))+'%', ha='center', va='bottom')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.title('Marital status distribution')\n",
        "plt.ylabel('number')\n",
        "\n",
        "\n",
        "#Plot Education level for each patients \n",
        "plt.subplot2grid((3,3),(1,0),colspan = 2) \n",
        "x_edu = hd_death.education.unique()\n",
        "plt.bar(x=x_edu, height=df2015.education.value_counts(), label='Total', color='grey', alpha=0.8)\n",
        "plt.bar(x=x_edu, height=hd_death.education.value_counts(), label='Heart Disease', color='lightgreen', alpha=0.8)\n",
        "\n",
        "for x in x_edu:\n",
        "    percent = round(hd_death.education.value_counts()[x]/df2015.education.value_counts()[x],2)\n",
        "    plt.text(x, hd_death.education.value_counts()[x] + 100,'%s' % str(int(percent*100))+'%', ha='center', va='bottom')\n",
        "plt.xticks(x_edu)\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.title('Educations distribution')\n",
        "plt.ylabel('number')\n",
        "\n",
        "#Plot Resident status of patients\n",
        "plt.subplot2grid((3,3),(1,2)) \n",
        "x_rs = hd_death.resident_status.unique()\n",
        "plt.bar(x=x_rs, height=df2015.resident_status.value_counts(), label='Total', color='grey', alpha=0.8)\n",
        "plt.bar(x=x_rs, height=hd_death.resident_status.value_counts(), label='Heart Disease', color='orange', alpha=0.8)\n",
        "for x in x_rs:\n",
        "    percent = round(hd_death.resident_status.value_counts()[x]/df2015.resident_status.value_counts()[x],2)\n",
        "    plt.text(x, hd_death.resident_status.value_counts()[x] + 150,'%s' % str(int(percent*100))+'%', ha='center', va='bottom')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.xticks(x_rs)\n",
        "plt.title('Resident status distribution')\n",
        "plt.ylabel('number')\n",
        "\n",
        "\n",
        "#Plot Race for patienst\n",
        "plt.subplot2grid((3,3),(2,0)) \n",
        "x_r = hd_death.race.unique()\n",
        "plt.bar(x=x_r, height=df2015.race.value_counts(), label='Total', color='grey', alpha=0.8)\n",
        "plt.bar(x=x_r, height=hd_death.race.value_counts(), label='Heart Disease', color='brown', alpha=0.8)\n",
        "for x in x_r:\n",
        "    percent = round(hd_death.race.value_counts()[x]/df2015.race.value_counts()[x],2)\n",
        "    plt.text(x, hd_death.race.value_counts()[x] + 150,'%s' % str(int(percent*100))+'%', ha='center', va='bottom')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.xticks(x_r)\n",
        "plt.title('Race distribution')\n",
        "plt.ylabel('number')\n",
        "\n",
        "#Plot Place of death for those patients \n",
        "plt.subplot2grid((3,3),(2,1)) \n",
        "x_p = hd_death.place_of_death.unique()\n",
        "plt.bar(x=x_p, height=hd_death.place_of_death.value_counts(), label='Total', color='grey', alpha=0.8)\n",
        "plt.bar(x=x_p, height=df2015.place_of_death.value_counts(), label='Heart Disease', color='gold', alpha=0.8)\n",
        "for x in x_p:\n",
        "    percent = round(hd_death.place_of_death.value_counts()[x]/df2015.place_of_death.value_counts()[x],2)\n",
        "    plt.text(x, hd_death.place_of_death.value_counts()[x] + 150,'%s' % str(int(percent*100))+'%', ha='center', va='bottom')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.xticks(x_p,rotation = -45)\n",
        "plt.title('place_of_death distribution')\n",
        "plt.ylabel('number')\n",
        "\n",
        "\n",
        "#Plot Quarters of death occurance \n",
        "plt.subplot2grid((3,3),(2,2))\n",
        "x_ss = hd_death.quarter.unique()\n",
        "plt.bar(x=x_ss, height=hd_death.quarter.value_counts(), label='Total', color='grey', alpha=0.8)\n",
        "plt.bar(x=x_ss, height=hd_death.quarter.value_counts(), label='Quarter', color='lightblue', alpha=0.8)\n",
        "\n",
        "for x in x_ss:\n",
        "    percent = round(hd_death.quarter.value_counts()[x]/df2015.quarter.value_counts()[x],2)\n",
        "    plt.text(x, hd_death.quarter.value_counts()[x] + 150, '%s' % str(int(percent*100))+'%', ha='center', va='bottom')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.title('Quarter distribution')\n",
        "plt.xticks(x_ss)\n",
        "plt.ylabel('number')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ot-0auiLBfUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cp_df1\n"
      ],
      "metadata": {
        "id": "jcaGMW3T3L1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cp_df1 data set contains estimaation of population n per each zip code/county state/city along with charcterstics of social determinanat factors \n",
        "from tabulate import tabulate\n",
        "\n",
        "print(tabulate(cp_df1[2:10], headers = cp_cols))"
      ],
      "metadata": {
        "id": "MTehv-XJ7lSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a copy of cp_df1 for further computation \n",
        "cp = cp_df1\n",
        "\n",
        "#Create new feature called socio to select a certain valuess in 'TITLE'variable/feature and we need only 'GEOID' and 'EST_1418' to be presented \n",
        "socio = (cp.loc[cp.TITLE.isin(['Employed', 'Unemployed', 'Unemployement Rate', 'Median household income (dollars)', 'Mean household income (dollars)', 'No health insurance coverage'])\n",
        ",['TITLE','GEOID', 'EST_1418']])\n",
        "\n",
        "socio.head()"
      ],
      "metadata": {
        "id": "Uq6V1KSqnCa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename columns just for handy working\n",
        "socio['TITLE'] = socio['TITLE'].replace({'Median household income (dollars)':'median_incom','Mean household income (dollars)':'mean_incom', 'No health insurance coverage':'no_health_insur'})\n",
        "socio.head()"
      ],
      "metadata": {
        "id": "Vl0GNbNTWSLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shape \n",
        "socio.shape"
      ],
      "metadata": {
        "id": "dGCJG932g-ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split 'GEOID' from 05000US01001 into the last for digits to match the 'FIPS' columns ebfore the merge \n",
        "socio['GEOID']=socio['GEOID'].astype(str)\n",
        "# #step 1: exclude 'US' string\n",
        "socio[\"GEOID\"] = socio[\"GEOID\"].apply(lambda x: x.split(\"US\")[1])\n",
        "# #Step2: \n",
        "socio[\"GEOID\"] = socio[\"GEOID\"].apply(lambda x: str(int(x)))\n",
        "\n",
        "socio.head()"
      ],
      "metadata": {
        "id": "yYhu_L3zqnox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove ',' and '.' from the 'GEOID'\n",
        "import re\n",
        "for c in socio.columns:\n",
        "\tsocio[c] = socio[c].apply(lambda x: x.replace(\",\",\"\") if isinstance(x, str) else x)\n",
        "for c in socio.columns:\n",
        "\tsocio[c] = socio[c].apply(lambda x: x.replace(\".\",\"\") if isinstance(x, str) else x)\n",
        "for c in socio.columns:\n",
        "\tsocio[c] = socio[c].apply(lambda x: x.replace(\"(X)\",\"\") if isinstance(x, str) else x)\n",
        "for c in socio.columns:\n",
        "\tsocio[c] = socio[c].apply(lambda x: x.replace(\"(X)\",\"\") if isinstance(x, str) else x)\n",
        "for c in socio.columns:\n",
        "\tsocio[c] = socio[c].apply(lambda x: x.replace(' ','') if isinstance(x, str) else x)\n",
        "\n",
        "socio[\"EST_1418\"].replace(' ','')\n",
        "socio[\"EST_1418\"].str.contains(r'\\S+').sum()\n",
        "# socio.head()"
      ],
      "metadata": {
        "id": "VVibRZgHrvnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "socio.shape"
      ],
      "metadata": {
        "id": "4kLOTeUgy08H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove nan\n",
        "socio[\"EST_1418\"] = socio[\"EST_1418\"].apply(lambda x: np.nan if x == \"N\" else x)\n",
        "\n",
        "socio[\"EST_1418\"] = socio[\"EST_1418\"].fillna(socio[\"EST_1418\"])\n",
        "\n",
        "#using special char\n",
        "special_characters = ['!','#','$','%', '&','@','[',']',' ',']','_','-']\n",
        "#After i checked, i found 8 row in the socio[socio['EST_1418'] has '', then i remove it.\n",
        "socio = socio[socio['EST_1418'] != '']\n",
        "\n",
        "#using for loop and replace to remove special characters\n",
        "for i in special_characters:\n",
        "    socio[\"EST_1418\"] = socio[\"EST_1418\"].replace(i,'')\n",
        "    \n",
        "# print final sample string    \n",
        "print(\"Final String:\",socio[\"EST_1418\"])\n"
      ],
      "metadata": {
        "id": "-f92HBAKYSOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert EST_1418 to float\n",
        "socio['EST_1418'] = socio['EST_1418'].astype(float)"
      ],
      "metadata": {
        "id": "k31psLcSmja7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Socio Features  "
      ],
      "metadata": {
        "id": "a5fh6lY0CImK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We have nan value, we need to compute impute before any chnage \n",
        "socio.head()"
      ],
      "metadata": {
        "id": "ehQjqVucJVee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature: average of household income for employed population\n",
        "employed = (socio.loc[socio.TITLE.isin(['Employed','mean_incom'])\n",
        ",['TITLE','GEOID', 'EST_1418']])\n",
        "\n",
        "employed_mean = employed.groupby('GEOID').EST_1418.agg(['count','mean']).reset_index()\n",
        "employed_mean\n",
        "#Rename columns\n",
        "employed_mean = employed_mean.rename(columns={'count':'employed_occurance','mean':'employed_avg_pop'})\n",
        "\n",
        "#Feature: average of household income for unemployed population\n",
        "unemployed = (socio.loc[socio.TITLE.isin(['Unemployed','mean_incom', 'no_health_insur'])\n",
        ",['TITLE','GEOID', 'EST_1418']])\n",
        "unemployed_mean = unemployed.groupby('GEOID').EST_1418.agg(['count','mean']).reset_index()\n",
        "#Rename columns\n",
        "unemployed_mean = unemployed_mean.rename(columns={'count':'unemployed_occurance','mean':'unemployed_avg_pop'})\n",
        "\n",
        "\n",
        "#Concate the new features\n",
        "employement_pop = pd.concat([employed_mean, unemployed_mean], axis=1)\n",
        "employement_pop"
      ],
      "metadata": {
        "id": "6m5UCBt9mH9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature: average of population with lack of health insurance\n",
        "uninsured_pop = (socio.loc[socio.TITLE.isin(['no_health_insur'])\n",
        ",['TITLE','GEOID', 'EST_1418']])\n",
        "uninsured_pop_avg = uninsured_pop.groupby('GEOID').EST_1418.agg(['count','mean']).reset_index()\n",
        "\n",
        "uninsured_pop_avg = uninsured_pop_avg.rename(columns={'count':'no_health_occurance','mean':'no_health_avg_pop'})\n",
        "uninsured_pop_avg"
      ],
      "metadata": {
        "id": "o6yH0dxWAXOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature: Average mean Income per each zip code\n",
        "income_avg = (socio.loc[socio.TITLE.isin(['mean_incom'])\n",
        ",['TITLE','GEOID', 'EST_1418']])\n",
        "income_mean = unemployed.groupby('GEOID').EST_1418.agg(['count','mean']).reset_index()\n",
        "#Rename columns\n",
        "income_mean = income_mean.rename(columns={'count':'incom_occurance','mean':'avg_Income'})\n",
        "income_mean"
      ],
      "metadata": {
        "id": "tV1h3K5ZOnpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check NA, Duplicate, and specific value\n",
        "# socio.dtypes\n",
        "(socio == '').sum()"
      ],
      "metadata": {
        "id": "JxRTs2UGBz4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# socio['EST_1418'] == socio['EST_1418']\n",
        "print(socio['TITLE']== 'mean_incom')"
      ],
      "metadata": {
        "id": "swYSxLkHzfU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Merge socio to df1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BA_OkLByqC2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Missing data\n",
        "# total = socio.isnull().sum().sort_values(ascending=False)\n",
        "# percent = (socio.isnull().sum()/socio.isnull().count()).sort_values(ascending=False)\n",
        "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "# missing_data.head(20)"
      ],
      "metadata": {
        "id": "OGHu6IjItwo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check duplicate rows\n",
        "socio.duplicated().sum()"
      ],
      "metadata": {
        "id": "gLTUoXO-ppwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exctracting duplicate rows\n",
        "socio.loc[socio.duplicated(),:]"
      ],
      "metadata": {
        "id": "ilV7x9Nns0XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Duplication Rows\n",
        "socio.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "sMi6woyrs_O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "socio.shape\n",
        "#We dropped 91 rows"
      ],
      "metadata": {
        "id": "Vy5dx7MAtSPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge Enemployed_mean\n",
        "df1 = df2015 #Make a copy to df2015\n",
        "\n",
        "#Convert GEOID and FIPS as Integer \n",
        "df1['FIPS']=df1['FIPS'].astype(int)\n",
        "employed_mean['GEOID']=employed_mean['GEOID'].astype(int)\n",
        "\n",
        "#Merge socio dataset to df1\n",
        "df1 = pd.merge(df1, employed_mean,  how='left', left_on=['FIPS'], right_on = ['GEOID'])"
      ],
      "metadata": {
        "id": "ZkHG-BmMQ3bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge Unemployed_mean\n",
        "df1 = df1 #Make a copy to df2015\n",
        "\n",
        "#Convert GEOID and FIPS as Integer \n",
        "df1['FIPS']=df1['FIPS'].astype(int)\n",
        "unemployed_mean['GEOID']=unemployed_mean['GEOID'].astype(int)\n",
        "\n",
        "#Merge socio dataset to df1\n",
        "df1 = pd.merge(df1, unemployed_mean,  how='left', left_on=['FIPS'], right_on = ['GEOID'])\n"
      ],
      "metadata": {
        "id": "2P8Lb7MUSLtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge income_avg\n",
        "df1 = df1 #Make a copy to df2015\n",
        "\n",
        "#Convert GEOID and FIPS as Integer \n",
        "df1['FIPS']=df1['FIPS'].astype(int)\n",
        "income_mean['GEOID']=income_mean['GEOID'].astype(int)\n",
        "\n",
        "#Merge socio dataset to df1\n",
        "df1 = pd.merge(df1, income_mean,  how='left', left_on=['FIPS'], right_on = ['GEOID'])"
      ],
      "metadata": {
        "id": "c72y0l-CSnoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge uninsured population\n",
        "df1 = df1 #Make a copy to df2015\n",
        "\n",
        "#Convert GEOID and FIPS as Integer \n",
        "df1['FIPS']=df1['FIPS'].astype(int)\n",
        "uninsured_pop_avg['GEOID']=uninsured_pop_avg['GEOID'].astype(int)\n",
        "\n",
        "#Merge socio dataset to df1\n",
        "df1 = pd.merge(df1, uninsured_pop_avg,  how='left', left_on=['FIPS'], right_on = ['GEOID'])\n"
      ],
      "metadata": {
        "id": "C-2B_PgcqlZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "wl0ZJ1dzRX7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print df1\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "zeZtX3NYv2aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "C7tl4UwmnWAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check duplicate rows\n",
        "df1.duplicated().sum()"
      ],
      "metadata": {
        "id": "lnsVN4EcuVzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have small proportion of duplication in the ros, i decided to drop it after investagtion\n",
        "df1.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "S16xKXVzUffd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## faclevel"
      ],
      "metadata": {
        "id": "9f-GcrYLCjzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#faclevel dataset \n",
        "fclevel_df1 = fl2021\n",
        "from tabulate import tabulate\n",
        "\n",
        "print(tabulate(fclevel_df1[1:10], headers = flcols))"
      ],
      "metadata": {
        "id": "uyj90o-SLcyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename and use lowercapital letters\n",
        "fclevel_df1 = fclevel_df1.rename(columns={'Provider Zip Code': 'zipcode', 'County': 'county',\n",
        "                                          'Provider City':'provider_city','Number of All Beds':'number_of_beds',\n",
        "                                          'Total Number of Occupied Beds':'total_occu_beds',\n",
        "                                          'Able to Test or Obtain Resources to Test All Current Residents Within Next 7 Days':'test_ava_resident',\n",
        "                                          'Able to Test or Obtain Resources to Test All Staff and/or Personnel Within Next 7 Days':'test_ava_staff',\n",
        "                                          'Shortage of Nursing Staff': 'shortage_nurse','Shortage of Clinical Staff':'shortage_staff',\n",
        "                                          'Number of Residents Staying in this Facility for At Least 1 Day This Week':'number_resident_staying_seven_days'})"
      ],
      "metadata": {
        "id": "MRl_CGEUuoz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #missing data\n",
        "# total = fclevel_df1.isnull().sum().sort_values(ascending=False)\n",
        "# percent = (fclevel_df1.isnull().sum()/fclevel_df1.isnull().count()).sort_values(ascending=False)\n",
        "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "# missing_data.head(20)"
      ],
      "metadata": {
        "id": "izxo6KNRxHeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fclevel_df1['test_ava_resident']== 'Y'"
      ],
      "metadata": {
        "id": "NM0ZmRH0nqHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have observed large proportion of missing value in:\n",
        "\n",
        "\n",
        "*   Number of Residents Staying in this Facility for At Least 1 Day This Week: 57%\n",
        "### We have observed small proportion of missing value in:\n",
        "*   Able to Test or Obtain Resources to Test All Staff and/or Personnel Within Next 7 Days: 14%\n",
        "*   Able to Test or Obtain Resources to Test All Current Residents Within Next 7 Days: 14%\n",
        "*   Shortage of Clinical Staff: ~2%\t\n",
        "*   Shortage of Nursing Staff: ~2%\n",
        "*   Number of All Beds: ~1%\n",
        "*   Total Number of Occupied Beds: ~1%\n",
        "\n"
      ],
      "metadata": {
        "id": "hxLwpQvVxS5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check duplicate rows\n",
        "fclevel_df1.duplicated().sum()"
      ],
      "metadata": {
        "id": "AyPgy0lkybTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exctracting duplicate rows\n",
        "fclevel_df1.loc[fclevel_df1.duplicated(),:]\n",
        "#After investgation the duplication is nothing could harm our data "
      ],
      "metadata": {
        "id": "GXt06JMOr1pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Willl not Drop Duplication Rows\n",
        "# fclevel_df1.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "5RraJuaSytD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove ',' and '.' from the 'GEOID'\n",
        "import re\n",
        "for c in fclevel_df1.columns:\n",
        "\tfclevel_df1[c] = fclevel_df1[c].apply(lambda x: x.replace(\",\",\"\") if isinstance(x, str) else x)\n",
        "for c in fclevel_df1.columns:\n",
        "\tfclevel_df1[c] = fclevel_df1[c].apply(lambda x: x.replace(\".\",\"\") if isinstance(x, str) else x)\n",
        "for c in fclevel_df1.columns:\n",
        "\tfclevel_df1[c] = fclevel_df1[c].apply(lambda x: x.replace(\"(X)\",\"\") if isinstance(x, str) else x)\n",
        "for c in fclevel_df1.columns:\n",
        "\tfclevel_df1[c] = fclevel_df1[c].apply(lambda x: x.replace(\"(X)\",\"\") if isinstance(x, str) else x)\n",
        "for c in fclevel_df1.columns:\n",
        "\tfclevel_df1[c] = fclevel_df1[c].apply(lambda x: x.replace(' ','') if isinstance(x, str) else x)\n",
        "for c in fclevel_df1.columns:\n",
        "\tfclevel_df1[c] = fclevel_df1[c].apply(lambda x: x.replace('NaN ','') if isinstance(x, str) else x)\n",
        "\n",
        "# socio[\"EST_1418\"].replace(' ','')\n",
        "# socio[\"EST_1418\"].str.contains(r'\\S+').sum()\n",
        "# socio.head()"
      ],
      "metadata": {
        "id": "G3ig6q0ZB9q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove nan\n",
        "fclevel_df1[\"number_of_beds\"] = fclevel_df1[\"number_of_beds\"].apply(lambda x: np.nan if x == \"NaN\" else x)\n",
        "\n",
        "fclevel_df1[\"number_of_beds\"] = fclevel_df1[\"number_of_beds\"].fillna(fclevel_df1[\"number_of_beds\"])\n",
        "\n",
        "#using special char\n",
        "special_characters = ['!','#','$','%', '&','@','[',']',' ',']','_','-']\n",
        "#After i checked, i found 8 row in the socio[socio['EST_1418'] has '', then i remove it.\n",
        "fclevel_df1 = fclevel_df1[fclevel_df1[\"number_of_beds\"] != \"NaN\"]"
      ],
      "metadata": {
        "id": "VKFdIJr4C06O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fclevel_df1.head()"
      ],
      "metadata": {
        "id": "4-0Iagziy8Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check NA\n",
        "# socio.dtypes\n",
        "(fclevel_df1 == 'NaN').sum()"
      ],
      "metadata": {
        "id": "Zm78BjY-sZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fclevel_df1.head()"
      ],
      "metadata": {
        "id": "qlq3-6D7yPpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  faclevel Feature "
      ],
      "metadata": {
        "id": "4EaUd1M-WTzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test_ava_resident profiling\n",
        "fclevel_df1['test_ava_resident'].loc[fclevel_df1['test_ava_resident'] == 'Y'] = 1 # Test available for resident\n",
        "fclevel_df1['test_ava_resident'].loc[fclevel_df1['test_ava_resident'] == 'N'] = 0 #Test not available for resident\n",
        "\n",
        "#test_ava_staff profiling\n",
        "fclevel_df1['test_ava_staff'].loc[fclevel_df1['test_ava_staff'] == 'Y'] = 1 # Test available for staff\n",
        "fclevel_df1['test_ava_staff'].loc[fclevel_df1['test_ava_staff'] == 'N'] = 0 #Test not available for staff\n",
        "\n",
        "#shortage_nurse profiling\n",
        "fclevel_df1['shortage_nurse'].loc[fclevel_df1['shortage_nurse'] == 'Y'] = 1 # Shortage in nurse \n",
        "fclevel_df1['shortage_nurse'].loc[fclevel_df1['shortage_nurse'] == 'N'] = 0 #Suffieceint sursing resources\n",
        "\n",
        "#shortage_staff profiling\n",
        "fclevel_df1['shortage_staff'].loc[fclevel_df1['shortage_staff'] == 'Y'] = 1 # Shortage in nurse \n",
        "fclevel_df1['shortage_staff'].loc[fclevel_df1['shortage_staff'] == 'N'] = 0 #Suffieceint sursing resources"
      ],
      "metadata": {
        "id": "nmSX2aV0cd7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shortage in test resources \n",
        "# fclevel_df1 = fclevel_df1[(fclevel_df1['test_ava_resident'] == 0) & (fclevel_df1['test_ava_staff'] == 0)]\n",
        "fclevel_df1.head()"
      ],
      "metadata": {
        "id": "FVHAMNb5hBUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortage in human resources\n",
        "# fclevel_df1 = fclevel_df1[(fclevel_df1['shortage_nurse'] == 1) & (fclevel_df1['shortage_staff'] == 1)]\n",
        "fclevel_df1"
      ],
      "metadata": {
        "id": "U9e-sn5TiO-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature:\n",
        "#Create avergae of total beds per each zip code\n",
        "beds_avg = fclevel_df1.groupby('zipcode').number_of_beds.agg(['count','mean']).reset_index()\n",
        "#Rename columns\n",
        "beds_avg = beds_avg.rename(columns={'count':'beds_occurance','mean':'beds_avg'})\n",
        "beds_avg"
      ],
      "metadata": {
        "id": "YxNRuG_cWd3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create avergae of beds per each zip code\n",
        "beds_occ_avg = fclevel_df1.groupby('zipcode').total_occu_beds.agg(['count','mean']).reset_index()\n",
        "#Rename columns\n",
        "beds_occ_avg = beds_occ_avg.rename(columns={'count':'beds_ocup_occurance','mean':'beds_occ_avg'})\n",
        "beds_occ_avg"
      ],
      "metadata": {
        "id": "EKlTMkN6XzgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create avergae of number_resident_staying_seven_days per each zip code\n",
        "avg_number_resident_staying_seven_days = fclevel_df1.groupby('zipcode').number_resident_staying_seven_days.agg(['count','mean']).reset_index()\n",
        "#Rename columns\n",
        "avg_number_resident_staying_seven_days = avg_number_resident_staying_seven_days.rename(columns={'count':'resi_staying_occurance','mean':'resi_staying_avg'})\n",
        "avg_number_resident_staying_seven_days"
      ],
      "metadata": {
        "id": "xMhPB_O6bRb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fclevel_df1.head()"
      ],
      "metadata": {
        "id": "N_pfIb-EbCh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(fclevel_df1 == '').sum()"
      ],
      "metadata": {
        "id": "v2OO11LXgkXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge 2"
      ],
      "metadata": {
        "id": "NXRihjWdY6pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge bed average and df1\n",
        "#Convert zipcode and FIPS as Integer \n",
        "\n",
        "df1 = df1\n",
        "# df1 = df1.iloc[:1000000]\n",
        "# fclevel_df1 = fclevel_df1.iloc[:1000000]\n",
        "\n",
        "\n",
        "df1['FIPS']=df1['FIPS'].astype(int)\n",
        "beds_avg['zipcode']=beds_avg['zipcode'].astype(int)\n",
        "\n",
        "#Merge socio dataset to df1\n",
        "df1 = pd.merge(df1, beds_avg,  how='left', left_on=['FIPS'], right_on = ['zipcode'])"
      ],
      "metadata": {
        "id": "kISbSzFGjOVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge beds_occ_avg and df1\n",
        "#Convert zipcode and FIPS as Integer \n",
        "\n",
        "df1 = df1\n",
        "# df1 = df1.iloc[:1000000]\n",
        "# fclevel_df1 = fclevel_df1.iloc[:1000000]\n",
        "\n",
        "\n",
        "df1['FIPS']=df1['FIPS'].astype(int)\n",
        "beds_occ_avg['zipcode']=beds_occ_avg['zipcode'].astype(int)\n",
        "\n",
        "#Merge socio dataset to df1\n",
        "df1 = pd.merge(df1, beds_occ_avg,  how='left', left_on=['FIPS'], right_on = ['zipcode'])"
      ],
      "metadata": {
        "id": "1cCZpnoTj1Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge avg_number_resident_staying_seven_days and df1\n",
        "#Convert zipcode and FIPS as Integer \n",
        "\n",
        "df1 = df1\n",
        "# df1 = df1.iloc[:1000000]\n",
        "# fclevel_df1 = fclevel_df1.iloc[:1000000]\n",
        "\n",
        "\n",
        "df1['FIPS']=df1['FIPS'].astype(int)\n",
        "avg_number_resident_staying_seven_days['zipcode']=avg_number_resident_staying_seven_days['zipcode'].astype(int)\n",
        "\n",
        "#Merge socio dataset to df1\n",
        "df1 = pd.merge(df1, avg_number_resident_staying_seven_days,  how='left', left_on=['FIPS'], right_on = ['zipcode'])"
      ],
      "metadata": {
        "id": "sxrWGm6OkHm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.FIPS.unique().shape"
      ],
      "metadata": {
        "id": "sB1g8Vw4jssr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dropna().shape"
      ],
      "metadata": {
        "id": "1gAsK3qNNYDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dropna()"
      ],
      "metadata": {
        "id": "bUHYMNPiNu_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dropna()"
      ],
      "metadata": {
        "id": "reEZbEEdNn4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.duplicated().sum()"
      ],
      "metadata": {
        "id": "B2N8wKNFBUni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isnull()"
      ],
      "metadata": {
        "id": "iRgU8QUWL03d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dropna(how='all')"
      ],
      "metadata": {
        "id": "cPY_JN4KTknX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isnull().all()"
      ],
      "metadata": {
        "id": "vmk-9tO4UB4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dropna()"
      ],
      "metadata": {
        "id": "V8wjSaqBMw25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plc_df1"
      ],
      "metadata": {
        "id": "was9PkegDHJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plc_df1.head()"
      ],
      "metadata": {
        "id": "ZEihTKZUJSuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### health_socio Feature "
      ],
      "metadata": {
        "id": "BvLWeA4Hk5ZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Condition used for health profiling\n",
        "*   Diagnosed diabetes among adults aged >=18 years\n",
        "*   Coronary heart disease among adults aged >=18 years\n",
        "*   Cholesterol screening among adults aged >=18 years\n",
        "*   Binge drinking among adults aged >= 18 years\n",
        "*   Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euuu6NIVlYjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new feature called 'risk_factor' include those values associated with heart_disease\n",
        "\n",
        "plc_df1['risk_factor'] = ''\n",
        "plc_df1['risk_factor'].loc[(plc_df1['Measure']=='Coronary heart disease among adults aged >=18 years')|\n",
        "                      (plc_df1['Measure']=='Diagnosed diabetes among adults aged >=18 years')|\n",
        "                      (plc_df1['Measure']=='Cholesterol screening among adults aged >=18 years')|\n",
        "                      (plc_df1['Measure']=='Binge drinking among adults aged >= 18 years')|\n",
        "                      (plc_df1['Measure']=='Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure')] = 1"
      ],
      "metadata": {
        "id": "s64iEeP1yaSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename and use lowercapital letters\n",
        "plc_df2 = plc_df1.rename(columns={'risk_factor': 'risk_factor', 'Year': 'year','LocationID':'locationid'})"
      ],
      "metadata": {
        "id": "GlO2coroXQIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plc_df2 = plc_df2[['risk_factor', 'year', 'locationid']]\n",
        "\n",
        "#One more thing to filter by year ==2015\n",
        "plc_df2['year'].loc[(plc_df2['year']=='2015')]\n",
        "plc_df2.head()\n"
      ],
      "metadata": {
        "id": "0GZqulDcd19y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #missing data\n",
        "# total = plc_df2.isnull().sum().sort_values(ascending=False)\n",
        "# percent = (plc_df2.isnull().sum()/plc_df2.isnull().count()).sort_values(ascending=False)\n",
        "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "# missing_data.head(20)\n",
        "plc_df2.shape"
      ],
      "metadata": {
        "id": "cMR5KXCC-C7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop the NA values \n",
        "# plc_df2.dropna(axis = 0, how = 'any', inplace = True)\n",
        "(plc_df2 == 'NaN').sum()"
      ],
      "metadata": {
        "id": "nV0oXR47-Zws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Socio Econ Feature\n",
        "\n",
        "\n",
        "*   Current lack of health insurance among adults aged 18-64 years\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "nVCnOwvtn4Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "CvMrIfkUXPbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge 3"
      ],
      "metadata": {
        "id": "BxT3TxZUZUfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Check if there are Null value \n",
        "print(plc_df2[plc_df2['year']==2019])"
      ],
      "metadata": {
        "id": "EN_SfqErGRQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Then for removing all non-numeric values use to_numeric with parameter errors='coerce' - to replace non-numeric values to NaNs:\n",
        "plc_df2['locationid'] = pd.to_numeric(plc_df2['locationid'], errors='coerce')\n",
        "\n",
        "# #Then for removing all non-numeric values use to_numeric with parameter errors='coerce' - to replace non-numeric values to NaNs:\n",
        "plc_df2['locationid'] = pd.to_numeric(plc_df2['locationid'], errors='coerce')\n",
        "\n",
        "# #And for remove all rows with NaNs in column x use dropna:\n",
        "plc_df2 = plc_df2.dropna(subset=['locationid'])\n",
        "\n",
        "# #Double Check if there are Null value \n",
        "print(plc_df2[plc_df2['locationid'].isnull()])\n",
        "\n",
        "# #Convert 'LocationID' from float to Int\n",
        "plc_df2['locationid']=plc_df2['locationid'].astype(int)\n",
        "plc_df2.dtypes\n",
        "\n",
        "# df1.update(health_socio)\n",
        "# #Merge df1'FIPS' and health_socio'LocationID'\n",
        "df1 = pd.merge(df1, plc_df2,  how='left', left_on=['FIPS'], right_on = ['locationid'])\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "yKxEHgUWThHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['beds_avg'].isna().all()"
      ],
      "metadata": {
        "id": "beNKNQCePeVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plc_df2[plc_df2[\"locationid\"] == 12105]"
      ],
      "metadata": {
        "id": "9GNLEHCA5g5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Double Check if there are Null value \n",
        "# print(df1[df1['beds_avg'].isnull()])\n",
        "# #Limit the dataframe for compution purposes \n",
        "df1 = df1.iloc[:1000000]"
      ],
      "metadata": {
        "id": "WgW9glI6FIEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Dataset"
      ],
      "metadata": {
        "id": "a6gH_rdGh9mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "mIxAZOrvwtvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "zxxsKuxgLf8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We need the follwing features \n",
        "df1 = df1[['resident_status', 'sex', 'age', 'place_of_death','marital_status', 'current_data_year', 'manner_of_death', 'autopsy', 'education', 'race','FIPS', 'quarter', 'heart_disease',\n",
        "          'employed_avg_pop','unemployed_avg_pop','avg_Income','beds_avg','beds_occ_avg','resi_staying_avg', 'risk_factor', 'no_health_avg_pop']]"
      ],
      "metadata": {
        "id": "CoLUg8u-g7Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "11wpxX2NKhrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df1 == 'NaN').sum()"
      ],
      "metadata": {
        "id": "6zcxVFmxkFH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "TNAihi1VsIAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df1"
      ],
      "metadata": {
        "id": "ED8b3n7fEdhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputaion \n",
        "meanVal = np.nanmean(df[\"employed_avg_pop\"])\n",
        "df[\"employed_avg_pop\"] = df[\"employed_avg_pop\"].apply(lambda x: meanVal if np.isnan(x) else x)\n",
        "\n",
        "df.drop(columns=[\"beds_avg\", 'beds_occ_avg','resi_staying_avg'], inplace=True)\n"
      ],
      "metadata": {
        "id": "ljw8KvEQAuSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data\n",
        "# total = df1.isnull().sum().sort_values(ascending=False)\n",
        "# percent = (df1.isnull().sum()/df1.isnull().count()).sort_values(ascending=False)\n",
        "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "# missing_data.head(20)\n",
        "\n",
        "# df1 = df1.dropna()"
      ],
      "metadata": {
        "id": "dMdxWL8Se-IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df1.dropna(axis=0, how='all'))\n",
        "df1['age']"
      ],
      "metadata": {
        "id": "2rGq65IDsbVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep trainign and test "
      ],
      "metadata": {
        "id": "8HHQBB7b-EQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename \n",
        "df = df.rename(columns={'heart_disease':'disease'})"
      ],
      "metadata": {
        "id": "q50IpfTS3c1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['age'].head()"
      ],
      "metadata": {
        "id": "QdHjQ-hI0-ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #One hot encoding \n",
        "#encoding to be df1\n",
        "# Do one-hot encoding for marital_status.\n",
        "df['marital_status'] = df['marital_status'].astype(str)\n",
        "one_hot1 = pd.get_dummies(df['marital_status'],prefix = 'marital_status')\n",
        "df = df.drop(['marital_status'],axis = 1)\n",
        "df = pd.concat([df,one_hot1],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for marital_status.\n",
        "df['resident_status'] = df['resident_status'].astype(str)\n",
        "one_hot1 = pd.get_dummies(df['resident_status'],prefix = 'resident_status')\n",
        "df = df.drop(['resident_status'],axis = 1)\n",
        "df = pd.concat([df,one_hot1],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for place_of_death.\n",
        "df['place_of_death'] = df['place_of_death'].astype(str)\n",
        "one_hot2 = pd.get_dummies(df['place_of_death'],prefix = 'place_of_death')\n",
        "df = df.drop(['place_of_death'],axis = 1)\n",
        "df = pd.concat([df,one_hot2],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for manner_of_death.\n",
        "df['manner_of_death'] = df['manner_of_death'].astype(str)\n",
        "one_hot3 = pd.get_dummies(df['manner_of_death'],prefix = 'manner_of_death')\n",
        "df = df.drop(['manner_of_death'],axis = 1)\n",
        "df = pd.concat([df,one_hot3],axis = 1)\n",
        "\n",
        "\n",
        "# Do one-hot encoding for sex.\n",
        "df['sex'] = df['sex'].astype(str)\n",
        "one_hot5 = pd.get_dummies(df['sex'],prefix = 'sex')\n",
        "df = df.drop(['sex'],axis = 1)\n",
        "df = pd.concat([df,one_hot5],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for race.\n",
        "df['race'] = df['race'].astype(str)\n",
        "one_hot6 = pd.get_dummies(df['race'],prefix = 'race')\n",
        "df = df.drop(['race'],axis = 1)\n",
        "df = pd.concat([df, one_hot6],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for education.\n",
        "df['education'] = df['education'].astype(str)\n",
        "one_hot7 = pd.get_dummies(df['education'],prefix = 'education')\n",
        "df = df.drop(['education'],axis = 1)\n",
        "df = pd.concat([df, one_hot7],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for quarter.\n",
        "df['quarter'] = df['quarter'].astype(str)\n",
        "one_hot8 = pd.get_dummies(df['quarter'],prefix = 'quarter')\n",
        "df = df.drop(['quarter'],axis = 1)\n",
        "df = pd.concat([df, one_hot8],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for injury_at_work\n",
        "df['autopsy'] = df['autopsy'].astype(str)\n",
        "one_hot8 = pd.get_dummies(df['autopsy'],prefix = 'autopsy')\n",
        "df = df.drop(['autopsy'],axis = 1)\n",
        "df = pd.concat([df, one_hot8],axis = 1)\n",
        "\n",
        "# Do one-hot encoding for TITLE\n",
        "df['risk_factor'] = df['risk_factor'].astype(str)\n",
        "one_hot8 = pd.get_dummies(df['risk_factor'],prefix = 'risk_factor')\n",
        "df = df.drop(['risk_factor'],axis = 1)\n",
        "df = pd.concat([df, one_hot8],axis = 1)"
      ],
      "metadata": {
        "id": "oF3XYt9Dnh1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "oU-VhV7XDldq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['age']==48"
      ],
      "metadata": {
        "id": "DMupL_MW3sPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Drop some features with lower sig. value\n",
        "df = df.drop(['FIPS','marital_status_S','resident_status_4', 'place_of_death_2', 'place_of_death_4',\n",
        "                  'place_of_death_5','place_of_death_7','place_of_death_9','manner_of_death_Accidents', 'manner_of_death_5.0'],axis = 1)"
      ],
      "metadata": {
        "id": "3Z1jmj42oPo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change to float.\n",
        "df = df.astype(np.float32)\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "eXGihxAIQFV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['age'].head()"
      ],
      "metadata": {
        "id": "cLBAq9bV4UWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "disease = df[\"disease\"].to_numpy()\n",
        "age = df[\"age\"].to_numpy()\n",
        "df.drop(columns=[\"disease\", \"age\"], inplace=True)\n",
        "df[\"disease\"] = disease\n",
        "df[\"age\"] = age"
      ],
      "metadata": {
        "id": "JSDWpKg42jIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "p_06WXcR4dpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['age']"
      ],
      "metadata": {
        "id": "hCJvi8m4y6vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "khRcR3tel5rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Task Learning (MTL) - Disease & Age"
      ],
      "metadata": {
        "id": "rT5X4hlM1S_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "A0T9YuapKXR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.layers import MSELossFlat, CrossEntropyFlat \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "YVqwJWqy6vaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial dataset (to replace with the real dataset)"
      ],
      "metadata": {
        "id": "ygWf96CoKZJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import data"
      ],
      "metadata": {
        "id": "NfNeGyWiKhAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "sXtHOpk4W-mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "G5ym3ToYYARO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'education_High school graduate (includes equivalency)':'high_school'})"
      ],
      "metadata": {
        "id": "TPmTSNJhWukf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "nzu2vxOyrIM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #- 1: create a dictionnary called dic_disease = {0: \"No heart disease\", 1: \"Heart disease\"} ?\n",
        "# - 2: rename your df1 into df. Place all your features at the beginning and \"disease\" + \"age\" should be the last two columns.\n",
        "# -3: Keep the last lines (nb_diseases = len(np.unique .......) and below)\n",
        "dic_disease = {0: \"No heart disease\",\n",
        "               1: \"Heart disease\"}\n",
        "\n",
        "nb_diseases = len(np.unique(df[\"disease\"]))\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_valid = train_test_split(df, test_size=0.2, shuffle=True,\n",
        "                                      random_state=42)\n",
        "nb_features = len(df.columns) - 2\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "phpZGxE9c__R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Dataset object"
      ],
      "metadata": {
        "id": "jxV3SuwEKiWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.tensor(self.df.values[idx,:nb_features], dtype=torch.float32)\n",
        "    labels = self.df.values[idx,nb_features:]\n",
        "    disease = torch.tensor(int(labels[0]), dtype=torch.int64)\n",
        "    age = torch.tensor(float(labels[1]), dtype=torch.float32)\n",
        "    \n",
        "    return x, (disease, age.log_().div(4.75))  # age.log_() / 4.75 to obtain a value between 0 and 1\n",
        "\n",
        "\n",
        "  def show(self,idx):\n",
        "    x, y = self.__getitem__(idx)\n",
        "    disease, age = y\n",
        "    print(\"Disease: {}, Age: {}\".format(dic_disease[disease.item()], int(age.mul_(4.75).exp_().item())))"
      ],
      "metadata": {
        "id": "vNWyOJE91T0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the DataBunch"
      ],
      "metadata": {
        "id": "SQv0gMfNKmY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 128\n",
        "num_workers = 2"
      ],
      "metadata": {
        "id": "5FZQeQZhKr1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = MultiTaskDataset(df_train)\n",
        "valid_ds = MultiTaskDataset(df_valid)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                      num_workers=num_workers)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True,\n",
        "                      num_workers=num_workers)\n",
        "data = DataBunch(train_dl, valid_dl)"
      ],
      "metadata": {
        "id": "myWVmt1_7L4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example to retrieve the information of a patient using the id"
      ],
      "metadata": {
        "id": "n-HHq1klK5gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.show(1)"
      ],
      "metadata": {
        "id": "pnYMGksP-Mjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.__getitem__(0)"
      ],
      "metadata": {
        "id": "eUAxUH_PHdRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Task Model"
      ],
      "metadata": {
        "id": "PgWLNX3MLGu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "QhjPUtnfLPsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskModel(nn.Module):\n",
        "  def __init__(self, nb_diseases):\n",
        "    super(MultiTaskModel, self).__init__()\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(nb_features, 200)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(200, 10)\n",
        "\n",
        "    # create one head per task :)\n",
        "    self.fc1 = torch.nn.Linear(10, nb_diseases)\n",
        "    self.fc2 = torch.nn.Linear(10, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    disease = torch.sigmoid(self.fc1(x)) \n",
        "    age = torch.sigmoid(self.fc2(x))  # age log between 0 and 1\n",
        "    return [disease, age]"
      ],
      "metadata": {
        "id": "9asbqOO57pta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ],
      "metadata": {
        "id": "TaJDYwtvLQ5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskLossWrapper(nn.Module):\n",
        "  def __init__(self, task_num):\n",
        "    super(MultiTaskLossWrapper, self).__init__()\n",
        "    self.task_num = task_num\n",
        "    self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
        "\n",
        "  def forward(self, preds, disease, age):\n",
        "\n",
        "    mse, crossEntropy = MSELossFlat(), CrossEntropyFlat()\n",
        "\n",
        "    loss0 = crossEntropy(preds[0], disease)\n",
        "    loss1 = mse(preds[1], age)\n",
        "\n",
        "    precision0 = torch.exp(-self.log_vars[0])\n",
        "    loss0 = precision0 * loss0 + self.log_vars[0]\n",
        "\n",
        "    precision1 = torch.exp(-self.log_vars[1])\n",
        "    loss1 = precision1 * loss1 + self.log_vars[1]\n",
        "    \n",
        "    return loss0 + loss1"
      ],
      "metadata": {
        "id": "2fj7WeRM8D2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the metrics and build the Learner"
      ],
      "metadata": {
        "id": "Imkf69MqLR_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_disease(preds, disease, age):\n",
        "  return accuracy(preds[0], disease)\n",
        "def rmse_age(preds, disease, age):\n",
        "  return root_mean_squared_error(preds[1], age)\n",
        "\n",
        "metrics = [acc_disease, rmse_age]\n",
        "\n",
        "model = MultiTaskModel(nb_diseases)\n",
        "\n",
        "task_num = 2  # number of tasks\n",
        "loss_func = MultiTaskLossWrapper(task_num).to(data.device)  # just making sure the loss is on the gpu\n",
        "\n",
        "learn = Learner(data, model, loss_func=loss_func,\n",
        "                callback_fns=ShowGraph, metrics=metrics)\n",
        "\n",
        "# spliting the model (to use discriminative learning rates)\n",
        "learn.split([learn.model.linear1,\n",
        "             learn.model.linear2,\n",
        "             nn.ModuleList([learn.model.fc1, learn.model.fc2])]);\n",
        "\n",
        "# train only the heads first (last layers)\n",
        "learn.freeze()"
      ],
      "metadata": {
        "id": "1-HVBT3p8ZzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit"
      ],
      "metadata": {
        "id": "QdIYPY4ILaCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find learning rate"
      ],
      "metadata": {
        "id": "LCoP6XEeLcBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find learning rate\n",
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "metadata": {
        "id": "ydzzR9qRAYdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.recorder.losses"
      ],
      "metadata": {
        "id": "WZCkBQ_PAEdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# According to the graph, choose lr\n",
        "lr = 1e-5\n",
        "max_lr = 1e-2"
      ],
      "metadata": {
        "id": "AZUYBnHaAczo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit one cycle"
      ],
      "metadata": {
        "id": "AJvsi6waLojr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 15\n",
        "learn.fit_one_cycle(n_epoch, max_lr=max_lr,\n",
        "                    callbacks=[callbacks.SaveModelCallback(learn,\n",
        "                                                           every=\"improvement\",\n",
        "                                                           monitor=\"valid_loss\",\n",
        "                                                           name=\"stage-1\")])"
      ],
      "metadata": {
        "id": "SNgmMd_kLqBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjust learning rate"
      ],
      "metadata": {
        "id": "6V3HDjqpMTYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.load(\"stage-1\")"
      ],
      "metadata": {
        "id": "9HKaVQUBMUvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "metadata": {
        "id": "7uPH6gS4MXjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit another cycle"
      ],
      "metadata": {
        "id": "DcHpQi6CMcG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_lr = slice(1e-6, 1e-1)\n",
        "n_epoch = 30"
      ],
      "metadata": {
        "id": "WOcnw6mgM63S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(n_epoch, max_lr=max_lr,\n",
        "                    callbacks=[callbacks.SaveModelCallback(learn,\n",
        "                                                           every=\"improvement\",\n",
        "                                                           monitor=\"valid_loss\",\n",
        "                                                           name=\"stage-2\")])"
      ],
      "metadata": {
        "id": "hXzNuv4fMgsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = learn.load(\"stage-2\")"
      ],
      "metadata": {
        "id": "BQi4UGJsMva_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model & make predictions"
      ],
      "metadata": {
        "id": "SLkKsk5eNR2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save/Load"
      ],
      "metadata": {
        "id": "6NoInGItOKeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = learn.model.cpu()  # Moving inference to the CPU\n",
        "torch.save(trained_model.state_dict(), \"saved_model\")  # save\n",
        "\n",
        "\"\"\"\n",
        "# To load:\n",
        "trained_model = MultiTaskModel(nb_diseases)\n",
        "trained_model.load_state_dict(torch.load(\"saved_model\"))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "px_ImmG2N44b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create predictor object"
      ],
      "metadata": {
        "id": "esV9tcdAOL1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor():\n",
        "  def __init__(self, model, dic_disease):\n",
        "    self.model = model\n",
        "    self.disease = dic_disease\n",
        "\n",
        "  def predict(self, x):\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    preds = self.model(x.unsqueeze(0))\n",
        "    disease = self.disease[torch.softmax(preds[0], 1).argmax().item()]\n",
        "    age = int(torch.exp(preds[1] * 4.75).item())\n",
        "    \n",
        "    return disease, age"
      ],
      "metadata": {
        "id": "fcRo3vzsOJxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = Predictor(trained_model, dic_disease)"
      ],
      "metadata": {
        "id": "O31Fm8I4Oc3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predictions"
      ],
      "metadata": {
        "id": "0GF3tNDnP5sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patient_id = 14\n",
        "unknown_patient = df.values[patient_id, :nb_features]#replace with  # extract just the features from the raw dataframe\n",
        "predictor.predict(unknown_patient)"
      ],
      "metadata": {
        "id": "4e9GGIeyP7eF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}